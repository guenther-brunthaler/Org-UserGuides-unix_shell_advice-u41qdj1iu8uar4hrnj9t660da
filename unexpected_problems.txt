Unexpected problems
===================
Guenther Brunthaler <gb@emgenxx69lwyn5ctlr4nl64ul.local>
v2021.65

Copyright (c) 2019 Guenther Brunthaler. All rights reserved.

This document is free documentation.
Distribution is permitted under the terms of the GFDL-1.3.


Aliases are dangerous
---------------------

POSIX shell aliases are substituted before variables or shell globs are expanded for the command's arguments.

Also, contrary to shell function invocations, aliases are expanded in-line and may convert a seemingly single simple command into a sequence of unrelated commands.

This can be a problem if data is piped into an alias without the user recognizing that fact: Only the first command in the alias will receive the redirection; any remaining expanded commands of the same alias will receive the normal standard input of the process.

Effects can get even weirder if an alias changes shell options temporarily which alter globbing or include "&". But even the inclusion of just ";" can create trouble unless the alias author considers all consequences.

Taken together, aliases can do some things normal shell functions can't, but none of them are normally useful.

If an attacker can inject an alias definition into a shell, he can alter the shell's normal operation in subtle ways and do things one thought would be impossible without modifying the shell binary.

Avoid aliases in scripts and use them only when absolutely necessary.

If in doubt whether an alias has been defined to override a critical command,

use

$ command thecommand

(but "command" may have also be redefined as an alias) or

$ \thecommand

The latter works, because alias substitution takes place before the "unnecessary" Backslash will be removed by the shell parser. (This trick has been learned from an article written by Simon Tatham.)


"stat" is not a POSIX utility
-----------------------------


"seq" is not a POSIX utility
----------------------------


"which" is not a POSIX utility
------------------------------

"command -v" is the closest portable approximation.


POSIX "bc"
----------

It has neither "else" nor "do". None of the x1, x2, x3 expressions in "for (x1; x2; x3)" can be omitted.

When returning an actual value from a function, the argument of "return" must be enclosed within round parentheses.

The "^" exponentiation operator does not exist.

There is no read() function.

There are no "print" or "halt", "limits" or "warranty" statements.

There is no "last" variable and neither "." as an alternative for it.

Relational expressions are only allowed in control expressions of conditional statements. They cannot produce "boolean values". Boolean operators "||", "&&", "!" do not exist either.

Only single lower-case letters are allowed as identifiers. However, variables, arrays and functions reside in different name spaces.

Only ANSI-C 89 comments "/* */" are supported, not shell comments "#".

GNU "bc" allows all these as extensions.

Mutually recursive functions do not work. At least not in GNU "bc".

Avoid spaces in numbers (like in "1 000 000"). Not all bc implementations support them.

Avoid passing arrays to functions. POSIX currently specifies no method how to pass an array as an actual parameter, even though it defines how to declare arrays as formal parameters. It is also unclear whether those arrays would be passed by value or by reference. Several existing implementations support passing arrays only by value.


File-descriptor numbers for redirections are limited
----------------------------------------------------

POSIX only guarantees that file decriptor numbers 0 through 9 must be available for shell redirection. This means that code such as

$ sh -c 'echo hello >& 12' 12<& 1

may be syntactically correct, but may still fail because 12 is greater than 9.

To illustrate the problem: The above command works fine when "sh" refers to "bash" or to "busybox ash", but fails when "sh" refers to "dash".

POSIX only reserves the file descriptor numbers 0, 1 and 2 for special purposes (stdin, stdout and stderr).

But I have seen comments that some shell implementations reserve additional file descriptors for special purposes, such as logging or debugging output. Usually, file descriptor 3 and maybe even 4 seem to be reserved in such cases.

Even though this is not backed by POSIX, it seems a good idea to only choose file-descriptors for use in a script in the following order: 5, 6, 7, 8, 9, 4, 3 or 9, 8, 7, 6, 5, 4, 3.

This should minimize the possibility for conflicts with file descriptor numbers reserved by a shell implementation.


Option parsing for the C compiler does not support switch clustering
--------------------------------------------------------------------

POSIX defines that the "c99" utility does not "need" to support option switch clustering, even though it may do so.

Which means one cannot rely on switch clustering for "c99".

That is, use

$ c99 -s -O -D NDEBUG file.c

rather than

$ c99 -sOD NDEBUG file.c

It seems reasonable to assume the same restriction for the older "cc" utility which is no longer defined by POSIX.

I recommend "command -v"-checking for the presence of "c99" first, falling back to "cc" if not found.


Providing option values in the same command-line argument is not portable
-------------------------------------------------------------------------

Use

$ c99 -D NDEBUG -I ~/my_includes -l m file.c

rather than

$ c99 -DNDEBUG -I~/my_includes -lm file.c


"head -n" might read more than just the specified number of lines
-----------------------------------------------------------------

It seems "head -n" uses normal buffered reads via fread() internally, which means it reads its input in fixed-size chunks and not necessarily line by line.

This means it is dangerous to do something like this to sort a CSV file except for its header line using the following command:

$ { head -n 1 && sort; } < unsorted.csv > sorted.csv

because "head" might throw away data already read into its buffer which "sort" will then be missing.

Better use the shell's built-in "read" command in such cases, which always reads its input line by line:

$ { IFS= read -r L && printf '%s\n' "$L" && sort; } < unsorted.csv > sorted.csv


"xargs" and arbitrary file lists
--------------------------------

The first problem is that "xargs" does not tolerate spaces in file names - it interprets them as argument separators.

This can easily fixed, however, by prefixing every character in the $file_list (containing one file per line) by a backslash:

$ sed "s/./\\\\&/g" < $file_list | xargs $some_command ...

However, there is a second problem which makes xargs almost useless with im many cases: It does not recognize empty file lists.

That is,

$ true | xargs echo running

will execute the command once without any arguments, although one might expect it would not execute the command at all.

Note that "xargs" can often be replaced by "find"'s "-exec ... +"-construct, especially when filenames are fed into "xargs". Consider replacing

$ ls *.txt | xargs cksum

with

$ find . ! -path . -prune -name '*.txt' -exec cksum {} +

which is not only as efficient as "xargs" but handles tricky filenames much better.


$SHELL does not need to be an absolute path
-------------------------------------------

On a "systemd"-based installation, I encountered a $SHELL containing just the string "bash", not "/bin/bash". Instead of something like

----
test -x "${SHELL:=/bin/sh}" || { echo 'invalid $SHELL!' >& 2; false || exit; }
"$SHELL" some_script.sh
----

better do this:

----
case ${SHELL:=/bin/sh} in
	/*) ;;
	*) SHELL=`command -v -- "$SHELL"`
esac
test -x "$SHELL" || { echo 'invalid $SHELL!' >& 2; false || exit; }
"$SHELL" some_script.sh
----
